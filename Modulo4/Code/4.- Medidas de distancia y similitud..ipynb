{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='../Imagenes/iteso.jpg' width=\"50\" height=\"100\"/></a>\n",
    "\n",
    "# <center> <font color= #000047> Módulo IV: Medidas de Distancia y Similitud. </font> </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que se tiene determinado el tipo de dato a utilizar, se pueden realizar una medida de similitud\n",
    "proponiendo el índice adecuado apropiado. \n",
    "\n",
    ">La primera y más importante es evitar la combinación de datos, esto debido a que cada tipo de dato presenta características propias que no comparten con los de otra naturaleza.\n",
    "\n",
    ">Un índice de similitud $\\delta_{i,j}$ es una medida de que tan parecido es un dato $i$ con otro $j$. Generalmente, las similaridades están acotadas en el rango de cero a uno; un aumento en la similaridad implica un aumento de la semejanza entre datos o variables, y toda similaridad de un dato consigo mismo debería ser igual al máximo valor posible, es decir, uno.\n",
    "\n",
    ">Las distancias en cambio disminuyen con un aumento del parecido, no son negativas y la distancia de un elemento consigo mismo es cero. Tanto las matrices de similaridades como las de distancias son simétricas; es decir, la distancia entre el individuo `a` y el `b` es la misma que entre el `b` y el `a`.\n",
    "\n",
    ">Dependiendo del método elegido para la ordenación, la clasificación, o el cálculo de índices de diversidad, así como de la escala de medición de los rasgos funcionales, la asociación entre los datos se expresará en términos de similaridad o distancia. Sin embargo, las similaridades pueden transformarse en distancias y viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el rango cero-uno, la similaridad $\\delta_{i,j}$ puede ser transformada a distancia $d_{i,j}$ de la siguientes\n",
    "formas:\n",
    "\n",
    "$$d_{i,j} = 1 - \\delta_{i,j}$$\n",
    "\n",
    "$$d_{i,j} = \\sqrt{1-\\delta_{i,j}}$$\n",
    "\n",
    "$$d_{i,j} = \\sqrt{\\delta_{i,j} -2\\delta_{i,j} + \\delta_{i,j}}$$\n",
    "\n",
    "$$d_{i,j} = -\\log{(\\delta_{i,j})}$$\n",
    "\n",
    ">El uso de índices de diversidad funcional basados en distancias, así como los métodos de\n",
    "clasificación y/o de ordenación requiere una comprensión de las propiedades de la escala de\n",
    "medición de los rasgos funcionales de las especies, y de las características de las medidas de\n",
    "semejanza asociadas a cada tipo de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud en Datos de Doble Estado o Binarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando la matriz de datos $X$, proviene de la observación de $n$ atributos que toman el valor 0 si la característica está ausente y el valor 1 si está presente, la información del grado de asociación entre cualquier par de individuos y puede representarse como una tabla de contingencia de 2x2 conocida conmunmente como **Matriz de Confusión**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             |               |               |$Muestra_j$   |         |\n",
    "|:-----------:| ------------- |:-------------:|:------------:|:-------:|\n",
    "|             |               |**Presente(1)**|**Ausente(0)**|         |\n",
    "|             |**Presente(1)**|        a      |       b      |`a+b`    |\n",
    "|$Muestra_i$  |**Ausente(0)** |        c      |       d      |`c+d`    |\n",
    "|             |               |      `a+c`    |     `b+d`    |`a+b+c+d`|     \n",
    "\n",
    "Donde $a$ es el número de caracteres presentes comunes, $b$ es el número de caracteres presente en\n",
    "$i$ pero no en $j$, $c$ es el número de caracteres presentes en $j$ pero no presentes en $i$ y d es número\n",
    "de caracteres no presentes en ambos datos. Para la matriz $X$ de dimensión $m × n$, es posible crear\n",
    "o construir $\\frac{n(n−1)}{2}$ Matrices de Confusión. \n",
    "\n",
    "Se han propuesto diversas medidas de similaridad que verifican estas propiedades, entre otros, Jaccard (1908), Rusell y Rao (1940), Sorensen (1948) y Sokal y Michener (1958). Sin embargo, existen similitudes que no verifican las propiedades de simetría y rango tales como la de Kulczynski (1970) acotada en el rango $[0,“ )$ y otros que expresan dependencia estocástica entre $x_i$ y $x_j$ como son las de Yule (1912) y la de Pearson (1926), acotadas en el rango $(-1,1)$, donde la mayor disimilaridad corresponde a $-1$, la similaridad total a $1$ y el valor 0 se asocia a la independencia estocástica.\n",
    "\n",
    "> No existe un criterio universal de cuándo usar una u otra similitud. Los diferentes autores que han abordado el tema coinciden en que la elección de una determinada similitud dependerá del peso que se desea dar a las frecuencias de $a$, $b$, $c$ y $d$, del tipo de datos que se quieran representar y de la situación experimental (Legendre y Legendre 1979, Gower y Legendre 1986)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Similaridad          |                                            | Simetría  | Rango         |\n",
    "|:-------------------:|:------------------------------------------:|:---------:|:--------------:|\n",
    "|Emparejamiento simple| $\\frac{a+d}{a+b+c+d}$                      |si         |[0,1]        |\n",
    "|Rogers y Tanimoto    | $\\frac{a+d}{a+2b+2c+d}$                    |si         |[0,1]        |\n",
    "|Hamman               | $\\frac{(a+d)-(b+c)}{a+2b+2c+d}$            |si         |[-1,1]      |\n",
    "|Yule                 | $\\frac{ad-bc}{ad+bc}$                      |si         |[-1,1]       |\n",
    "|Pearson              | $\\frac{ad-bc}{\\sqrt((a+c)(b+d)(a+b)(c+d))}$|si         |[-1,1]       |\n",
    "|Jaccard              | $\\frac{b+c}{a+b+c}$                        |No         |[0,1]        |\n",
    "|Russel y Rao         | $\\frac{a}{a+b+c+d}$                        |No         |[0,1]        |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "No hay que olvidar que cada índice de similitud tiene propiedades distintas por lo que hay que\n",
    "considerar el objetivo que se quiere obtener para elegir el índice adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud en Datos multi estado Nominales "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si las categorías para cada variable son codificadas por ejemplo, como: $0,1, 2, 3, . . . , k,$ el grado de\n",
    "asociación entre cualquier par de individuos $x_i$ y $x_j$ puede medirse a través de la expansión del\n",
    "emparejamiento simple que se expresará como:\n",
    "\n",
    "$$d_{ij}=\\frac{número\\_de\\_caracteres\\_coincidentes}{número\\_total\\_de\\_caracteres}$$\n",
    "\n",
    "No obstante, cuando el cero representa ausencia del carácter es recomendable ignorar el empate\n",
    "de ceros en forma similar como lo hace `Jaccard`.\n",
    "\n",
    "Para el tratamiento de las variables ‘indicadoras excluyentes’ pueden utilizarse dos estrategias: \n",
    "\n",
    ">Uso de variables `‘auxiliares’` (dummy) o desdoblamiento en tantas variables como estados posibles presentes de la categoría.\n",
    "\n",
    "En el caso de variables ‘auxiliares’ cada variable estará representada por tantas pseudo variables como número de estados diferentes menos uno. \n",
    "\n",
    "Así cada categoría tendrá asociado un perfil con un 1 en el estado en que se encuentre, estando el último estado representado solo por ceros.\n",
    "\n",
    ">Cuando se realiza el desdoblamiento de una variable nominal en todos sus posibles estados, se\n",
    "identifica la presencia o ausencia de cada estado del rasgo funcional en estudio, pero como estos\n",
    "estados son excluyentes cada especie tendrá un solo valor de presencia (1) y el resto serán ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud en Datos Multi Estado Ordinales \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables ordinales pueden considerarse como variables cuantitativas si la asignación del ranking refleja en cierta forma una diferencia entre los estados de la variable. Por\n",
    "ejemplo, si se considera la resistencia al fuego de un conjunto de muestras usando las categorías:\n",
    "`muy baja`, `baja`, `media`, `alta` y `muy alta`; puede ser razonable asignarle valores: 0, 1, 2, 3, 4,\n",
    "respectivamente ya que las categorías consecutivas pueden considerarse como equidistantes. De\n",
    "esta manera, la nueva variable numérica podría ser tratada como una variable cuantitativa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud en datos cuantitativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalizando para $m$ muestras y $n$ variables aleatorias cuantitativas (rasgos funcionales), la\n",
    "distancia usual que se observa entre el par de unidades $x_i$ y $x_j$ cuando se representan en el espacio\n",
    "de coordenadas definido por n variables cuantitativas, es conocida como distancia Euclideana:\n",
    "\n",
    "$$d_{i,j} = \\sqrt{\\sum_{t=1}^n (x_{it} - x_{jt})^2}$$\n",
    "\n",
    "donde $n$ son los rasgos funcionales. La distancia Euclídeana es la más conocida, la de mayor uso y\n",
    "es la herramienta fundamental de cálculo de la mayoría de los métodos multivariados basados en\n",
    "distancias. \n",
    "\n",
    "Sin embargo, presenta varios inconvenientes: \n",
    "\n",
    ">no está acotada, es sensible a cambios de\n",
    "escalas y considera las n variables estocásticamente independientes.\n",
    "\n",
    "Se han propuesto varias transformaciones que permiten minimizar y/o eliminar estos inconvenientes, entre otras: \n",
    "\n",
    ">se recomienda utilizarla en caso de homogeneidad entre la naturaleza física de las variables, cuando esto no es posible se puede estandarizar cada variable por su rango $r_t$ asegurando que la contribución de cualquier variable estará acotada en el intervalo (0,1).\n",
    "\n",
    "Además puede dividirse por la cantidad de variables obteniendo una distancia media que oscilará en este\n",
    "rango y facilita su conversión a similaridad, la expresión estará definida por:\n",
    "\n",
    "$$d_{i,j} = \\sqrt{\\frac{1}{n} \\sum_{t=1}^n \\frac{(x_{it} - x_{jt})^2}{r_t}}$$\n",
    "\n",
    "A continuación se presenta la formulación y propiedades de las distancias y disimilaridades no\n",
    "negativas más utilizadas en los estudios de iversidad. Las más usadas son las distancias: Euclídea, Manhattan y Mahalanobis.\n",
    "\n",
    "|Similaridad          |                                            | Simetría  | Rango         |\n",
    "|:-------------------:|:------------------------------------------:|:---------:|:--------------:|\n",
    "|Euclideana| $\\sqrt{ \\sum_{t=1}^n (x_{it} - x_{jt})^2}$                      |si         |[0,$\\infty$]        |\n",
    "|Manhattan    | $ \\sum_{t=1}^n |x_{it} - x_{jt}|$                  |si         |[0,$\\infty$]        |\n",
    "|Bray-Curtis               | $\\frac{ \\sum_{t=1}^n |x_{it} - x_{jt}|}{ \\sum_{t=1}^n (x_{it} + x_{jt})}$            |si         |[0,$\\infty$]      |\n",
    "|Canberra                 | $\\sum_{t=1}^n\\frac{ |x_{it} - x_{jt}|}{(|x_{it}| + |x_{jt}|)}$                       |si         |[0,$\\infty$]       |\n",
    "|Minkowski              | $^p\\sqrt{ \\sum_{t=1}^n (x_{it} - x_{jt})^p}$ |si         |[0,$\\infty$]        |\n",
    "|Mihalanobis              | $\\sqrt{ \\sum_{l=1}^n\\sum_{t=1}^n (x_{it} - x_{jt}) \\sigma_{lt}^{-1}(x_{il} - x_{jl})}$                       |si         |[0,$\\infty$]        |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canberra\n",
    "$$\\sum_i \\frac{|u_i - v_i|}{|u_i| + |v_i|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular las distancias por:\n",
    "# Distacia Jaccard\n",
    "# Distacia braycurtis\n",
    "# Manhatan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z= \\frac{x-\\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud por Coseno\n",
    "\n",
    "La función de similitud del coseno es:\n",
    "\n",
    "$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\\tag{1}$$\n",
    "\n",
    "$ A $ y $ B $ representan los vectores y $ A_i $ o $ B_i $ representan el índice i de ese vector.\n",
    "& Hay que tener en cuenta que si A y B son idénticos, se obtendrá $ cos (\\theta) = 1 $.\n",
    "\n",
    "* De lo contrario, si son totalmente opuestos, es decir, $ A = -B $, obtendría $ cos (\\theta) = -1 $.\n",
    "* Si obtiene $ cos (\\ theta) = 0 $, eso significa que son ortogonales (o perpendiculares).\n",
    "* Los números entre 0 y 1 indican una puntuación de similitud.\n",
    "* Los números entre -1-0 indican una puntuación de disimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deducción\n",
    "\n",
    "$$A\\cdot B = ||A||||B||cos(\\theta)$$\n",
    "\n",
    "$$\\frac{A\\cdot B}{||A||||B||} = cos(\\theta)= similaridad \\in [-1,1]$$\n",
    "\n",
    "$$d= 1 - similaridad = 1 - \\frac{A\\cdot B}{||A||||B||} \\in \\{0,2\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Qué pasa si mi dataset contiene todo tipo de datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos Mixtos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que sobre S muestras, representadas en la matriz $X$, se han observado simultáneamente diferentes muestras que por su naturaleza pueden corresponder a variables binarias,\n",
    "cualitativas y cuantitativas. \n",
    "\n",
    ">La distancia entre individuos que presenten esta combinación de características puede ser medida a través de la similaridad de `Gower (1971)`. Esta similaridad es útil para tipos de datos mixtos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La similaridad $\\delta_{ijt}$ entre la $i$-ésima y la $j$-ésima muestra para la variable $t$, promediado sobre las $T$ variables o características, es la medida de similaridad propuesta por Grower (1971):\n",
    "\n",
    "$$\\delta_{ij}=\\frac{1}{T} \\delta_{ijt}$$\n",
    "\n",
    ">En el caso de variables **binarias o cualitativas**, $\\delta_{ijt}=1$ si $x_{it}=x_{jt}$ y $\\delta_{ijt}=0$ si $x_{it}\\neq x_{jt}$. \n",
    "\n",
    ">Si la variable es **cuantitativa** la similaridad entre los individuos estará dada por:\n",
    "\n",
    "$$\\delta_{ijt}=1-\\frac{|x_{it} - x_{jt}|}{rt}$$\n",
    "\n",
    " donde $r_t$ es el rango (diferencia entre el máximo y el mínimo) de la $t$-ésima característica sobre toda la población. Si $x_{it}=x_{jt}$ entonces $\\delta_{ijt}=1$, cuando $x_{it}$ y $x_{jt}$ se encuentan en los extremos de $r_t$ entonces $\\delta_{ijt}=0$, y si $x_{it}$ y $x_{jt}$ son valores intermedios de $r_t$ entonces $0\\leq \\delta_{ijt} \\leq 1$\n",
    " \n",
    " \n",
    " El tratamiento de los datos faltantes, la consideración de la ausencia simultánea de la característica, y la ponderación relativa de características, son debilidades metodológicas que afectan las medidas de similitud y distancias referidas hasta ahora para todo tipo de variables. \n",
    " \n",
    " En este sentido, Gower introdujo en la expresión de la medida de similitud, ponderaciones denominadas $w_{ijt}$, como una función que\n",
    "depende de cada par de datos $x_{it}$ y $x_{jt}$ , obteniendo la siguiente expresión general:\n",
    "\n",
    "$$\\delta_{ij} = \\frac{\\sum_{t=1}^T w_{ijt}\\delta_{ijt}}{\\sum_{t=1}^T w_{ijt}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Encontrar la distancia Manhattan entre cada muestra para las variables numéricas\n",
    "2. Normalizar la matriz de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular la distancia de similitud para las variables categóricas. \n",
    "\n",
    "1. Primero hay que convertir las variables categoricas nominales a variables dummies y después calcular las distancias de similitud (DICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the matrix returns a value of 1 wherever if finds a non equal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gower_matrix in module gower.gower_dist:\n",
      "\n",
      "gower_matrix(data_x, data_y=None, weight=None, cat_features=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gower.gower_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos interpretar la matriz de la siguiente manera. En la primera columna, vemos la diferencia del primer cliente con todos los demás. Este cliente es similar al segundo, tercer y sexto cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
